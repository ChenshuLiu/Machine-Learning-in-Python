{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0abc41d",
   "metadata": {},
   "source": [
    "# Machine Learning in Python\n",
    "#### by: Chenshu Liu\n",
    "Reference link: https://youtu.be/rLOyrWV8gmA?list=PLGenESRtZKmFRJ7ZLXbhQRygnRwQOolXi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737ecfe0",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [KNN](#KNN)\n",
    "2. [Linear Regression](#Linear-Regression)\n",
    "3. [Logistic Regression](#Logistic-Regression)\n",
    "4. [Regression Refactoring](#Regression-Refactoring)\n",
    "5. [Naive Bayes](#Naive-Bayes)\n",
    "6. [Perceptron](#Perceptron)\n",
    "7. [SVM](#SVM)\n",
    "8. [Decision Tree](#Decision-Tree)\n",
    "9. [Random Forest](#Random-Forest)\n",
    "10. [PCA](#PCA)\n",
    "11. [K-Means](#K\\-Means)\n",
    "12. [AdaBoost](#AdaBoost)\n",
    "13. [LDA](#LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4eed1",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97831dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ecd8d",
   "metadata": {},
   "source": [
    "## KNN\n",
    "> KNN is the abreviation of K Nearest Neighbors. \n",
    "\n",
    "The process of determining the classification of the sample is find `k` (_defined by user_) nearest neighbors of the sample point (**e.g. the green point**) and calculate the euclidean distance of the neighbor points (**the blue and orange points**) from the sample points. \n",
    "\n",
    "Since we already know the categories that the neighbor points belong to, we can use the majority of the neighbor category as the classification for the sample point (_e.g. in the case of the image shown below, since two of the three points are blue, the sample point, i.e. the green point, is classified as blue_).\n",
    "\n",
    "![K-Nearest Neighbor](Images/KNN.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd07a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 1234)\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum(x1 - x2)**2)\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k = 3):\n",
    "        assert (k >= 1) & (type(k) == int), f\"k value {k} is invalid\"\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predicted_labels = [self._predict(x) for x in X]\n",
    "        return np.array(predicted_labels)\n",
    "        \n",
    "    def _predict(self, x):\n",
    "        # compute the distances\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        # get k nearest samples and labels\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = \n",
    "        # majority vote --> most common class label\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660accc",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3524aa",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc5c72",
   "metadata": {},
   "source": [
    "## Regression Refactoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb60a6",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b01181",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d2cb6",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e72b9",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b367daf",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c99502b",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6fece2",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b72c0",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4182bf",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a615d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdba345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8982ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead84f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
